{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using the Azure Data Lake Python SDK\n",
    "Amit Kukarni, a Program Manager on the Big Data team at Microsoft posted on medium about using Azure Data Lake in different ways. This is a modified excerpt of that post showing how to use Azure Data Lake from Azure Notebooks.\n",
    "\n",
    "Here is the [original post](https://medium.com/azure-data-lake/using-jupyter-notebooks-and-pandas-with-azure-data-lake-store-48737fbad305?platform=hootsuite)\n",
    "\n",
    "Here’s a question I hear every few days. How do I access data in the data lake store from my Jupyter notebooks? People generally want to load data that is in Azure Data Lake Store into a data frame so that they can analyze it in all sorts of ways.\n",
    "\n",
    "Sometimes you just want to run Jupyter in standalone mode and analyze all your data on a single machine. You simply want to reach over and grab a few files from your data lake store account to analyze locally in your notebook.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*tjyUdXYIXzsGLQKBdkRnwA.png)\n",
    "\n",
    "This is also fairly a easy task to accomplish using the Python SDK of Azure Data Lake Store. In this post I will show you all the steps required to do this.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "For the rest of this post, I assume that you have some basic familiarity with Python, Pandas and Jupyter.\n",
    "\n",
    "\n",
    "## Installing the Azure Data Lake Store Python SDK\n",
    "If you want to learn more about the Python SDK for Azure Data Lake store, the first place I will recommend you start is [here](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-get-started-python). Installing the Python SDK is really simple by running these commands to download the packages.\n",
    "\n",
    "\n",
    "This is very simple. I am assuming you have only one version of Python installed and pip is set up correctly. You simply need to run these commands and you are all set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install azure-mgmt-resource\n",
    "!pip install azure-mgmt-datalake-store\n",
    "!pip install azure-datalake-store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can validate that the packages are installed correctly by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azure-datalake-store (0.0.8)\n",
      "azure-mgmt-datalake-store (0.1.4)\n",
      "azure-mgmt-resource (1.0.0rc1)\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep 'azure-datalake-store\\|azure-mgmt-datalake-store\\|azure-mgmt-resource'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain an authentication token\n",
    "In order to read data from your Azure Data Lake Store account, you need to authenticate to it. There are multiple ways to authenticate. In this example below, let us first assume you are going to connect to your data lake account just as your own user account. The following method will work in most cases — even if your organization has enabled multi factor authentication and has Active Directory federation enabled.\n",
    "\n",
    "Running this in Jupyter will show you an instruction similar to the following. Click that URL and following the flow to authenticate with Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.datalake.store import core, lib, multithread\n",
    "token = lib.auth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an Azure Data Lake Store file into a Pandas data frame\n",
    "Once you go through the flow, you are authenticated and ready to access data from your data lake store account.\n",
    "\n",
    "If you run the below, you can get the data frame from your file in the data lake store account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an ADLS File System Client. The store_name is the name of your ADLS account\n",
    "adlsFileSystemClient = core.AzureDLFileSystem(token, store_name='amitadls')\n",
    "\n",
    "# Read a file into pandas dataframe\n",
    "with adlsFileSystemClient.open('earthquakes.csv', 'rb') as f:\n",
    "    df = pd.read_csv(f) \n",
    "\n",
    "# Show the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here onward, you can now panda-away on this data frame and do all your analysis.\n",
    "\n",
    "## Using a Service Principal Identity\n",
    "There is another way one can authenticate with the Azure Data Lake Store. That way is to use a service principal identity. A step by step tutorial for setting up an Azure AD application, retrieving the client id and secret and configuring access using the SPI is available here.\n",
    "Once you get all the details, replace the authentication code above with these lines to get the token. After you have the token, everything there onward to load the file into the data frame is identical to the code above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from azure.common.credentials import ServicePrincipalCredentials\n",
    "token = lib.auth(tenant_id = '<your azure tenant id>', client_secret = '<your client secret>', client_id = '<your client id>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "To round it all up, basically you need to install the Azure Data Lake Store Python SDK and thereafter it is really easy to load files from the data lake store account into your Pandas data frame. It works with both interactive user identities as well as service principal identities.\n",
    "Hopefully, this article helped you figure out how to get this working."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
